{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH3lLs0XSoyKDrf00ndvoe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavika2222/mission_success/blob/main/dl4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UcbqpzTV5la7"
      },
      "outputs": [],
      "source": [
        "#autoencoder anamoly detection\n",
        "#dataset present but no need to download from google and upload , it will download automatically by below path link.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
        "\n",
        "# Download the dataset\n",
        "path = '''http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'''\n",
        "data = pd.read_csv(path, header=None)\n",
        "print(data.shape)\n",
        "data.head()\n",
        "\n",
        "\n",
        "#Split the data for training and testing\n",
        "# last column is the target\n",
        "# 0 = anomaly, 1 = normal\n",
        "\n",
        "TARGET = 140\n",
        "\n",
        "features = data.drop(TARGET, axis=1)\n",
        "target = data[TARGET]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    features, target, test_size=0.2,\n",
        "    random_state = 0, stratify=target\n",
        ")\n",
        "x_test.shape\n",
        "x_train.shape\n",
        "target.value_counts()\n",
        "\n",
        "# use case is novelty detection so use only the normal data\n",
        "# for training\n",
        "\n",
        "train_index = y_train[y_train == 1].index\n",
        "train_data = x_train.loc[train_index]\n",
        "\n",
        "#Scale the data using MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "x_train_scaled = min_max_scaler.fit_transform(\n",
        "    train_data.copy())\n",
        "x_test_scaled = min_max_scaler.transform(x_test.copy())\n",
        "x_train.describe()\n",
        "pd.DataFrame(x_train_scaled).describe()\n",
        "\n",
        "#Build an AutoEncoder model\n",
        "# create a model by subclassing Model class in tensorflow\n",
        "class AutoEncoder(Model):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  output_units: int\n",
        "    Number of output units\n",
        "\n",
        "  code_size: int\n",
        "    Number of units in bottle neck\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_units, code_size=8):\n",
        "    super().__init__()\n",
        "    self.encoder = Sequential([\n",
        "      Dense(64, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(64, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
        "# configurations of model\n",
        "model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")\n",
        "\n",
        "#Plot history\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSLE Loss')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ]
    }
  ]
}